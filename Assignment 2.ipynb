{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T10:04:42.832359Z",
     "iopub.status.busy": "2025-12-01T10:04:42.830588Z",
     "iopub.status.idle": "2025-12-01T10:06:31.374543Z",
     "shell.execute_reply": "2025-12-01T10:06:31.372581Z",
     "shell.execute_reply.started": "2025-12-01T10:04:42.832294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: bertopic 0.17.3 does not provide the extra 'visualization'\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install --quiet \"bertopic[visualization]\" sentence-transformers umap-learn hdbscan gensim wordcloud plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T10:06:31.378601Z",
     "iopub.status.busy": "2025-12-01T10:06:31.378148Z",
     "iopub.status.idle": "2025-12-01T10:06:31.798748Z",
     "shell.execute_reply": "2025-12-01T10:06:31.797211Z",
     "shell.execute_reply.started": "2025-12-01T10:06:31.378565Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/521337484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# BERTopic & components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyBERTInspired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bertopic/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bertopic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bertopic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mhdbscan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHDBSCAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mHAS_HDBSCAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hdbscan/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhdbscan_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHDBSCAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrobust_single_linkage_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustSingleLinkage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobust_single_linkage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidity_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from .prediction import (approximate_predict,\n\u001b[1;32m      5\u001b[0m                          \u001b[0mmembership_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# SPDX-License-Identifier: BSD-3-Clause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ball_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVALID_METRICS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_METRICS_SPARSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_graph_by_row_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_ball_tree.pyx\u001b[0m in \u001b[0;36minit sklearn.neighbors._ball_tree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric64'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# BERTopic & components\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "# BERTopic visuals\n",
    "from bertopic.plotting import (\n",
    "    visualize_barchart,\n",
    "    visualize_hierarchy,\n",
    "    visualize_topics,\n",
    "    visualize_documents,\n",
    "    visualize_term_rank,\n",
    "    visualize_topics_over_time,\n",
    ")\n",
    "\n",
    "# Gensim utils (used internally by BERTopic.Coherence if needed)\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# -------------------------\n",
    "# USER KNOBS — adjust for lab/demo\n",
    "# -------------------------\n",
    "DATA_ROOT = r\"/kaggle/input/20-newsgroups/mini_newsgroups/mini_newsgroups\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST = 0.1\n",
    "UMAP_N_COMPONENTS = 5\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 40\n",
    "TOP_N_WORDS = 12\n",
    "CALCULATE_PROBABILITIES = False\n",
    "RANDOM_STATE = 42\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def load_texts_simple(root_dir, max_files=None, min_tokens=6):\n",
    "    p = Path(root_dir)\n",
    "    docs, paths = [], []\n",
    "    for file in p.rglob(\"*\"):\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "        try:\n",
    "            text = file.read_text(encoding=\"utf8\", errors=\"replace\").strip()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not text:\n",
    "            continue\n",
    "        if len(text.split()) < min_tokens:\n",
    "            continue\n",
    "        docs.append(text)\n",
    "        paths.append(str(file))\n",
    "        if max_files and len(docs) >= max_files:\n",
    "            break\n",
    "    return docs, paths\n",
    "\n",
    "# -------------------------\n",
    "# Light preprocessing: remove headers, urls, emails, signatures\n",
    "# (keeps natural text so embeddings remain meaningful)\n",
    "# -------------------------\n",
    "def light_preprocess(text):\n",
    "    text = re.sub(r\"(^|\\n)(from|subject|to|cc|reply-to|message-id):.*(\\n|$)\", \" \", text, flags=re.I)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[-]{2,}.*\", \" \", text)   # crude signature removal\n",
    "    text = re.sub(r\"[^A-Za-z0-9'\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "docs, paths = load_texts_simple(DATA_ROOT, max_files=None, min_tokens=6)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Apply light preprocessing\n",
    "# -------------------------\n",
    "print(\"Applying light preprocessing...\")\n",
    "docs_clean = [light_preprocess(d) for d in tqdm(docs)]\n",
    "\n",
    "# -------------------------\n",
    "# Embeddings (precompute)\n",
    "# -------------------------\n",
    "print(\"Loading SentenceTransformer:\", EMBEDDING_MODEL)\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "print(\"Computing embeddings...\")\n",
    "embeddings = embedder.encode(docs_clean, show_progress_bar=True, convert_to_numpy=True)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# -------------------------\n",
    "# UMAP reduction\n",
    "# -------------------------\n",
    "print(\"Fitting UMAP...\")\n",
    "umap_model = UMAP(n_neighbors=UMAP_N_NEIGHBORS,\n",
    "                  n_components=UMAP_N_COMPONENTS,\n",
    "                  min_dist=UMAP_MIN_DIST,\n",
    "                  metric=\"cosine\",\n",
    "                  random_state=RANDOM_STATE)\n",
    "embeddings_reduced = umap_model.fit_transform(embeddings)\n",
    "print(\"UMAP reduced shape:\", embeddings_reduced.shape)\n",
    "\n",
    "# -------------------------\n",
    "# HDBSCAN clustering\n",
    "# -------------------------\n",
    "print(\"Fitting HDBSCAN...\")\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "                                metric='euclidean',\n",
    "                                cluster_selection_method='eom',\n",
    "                                prediction_data=True)\n",
    "labels = hdbscan_model.fit_predict(embeddings_reduced)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = int((labels == -1).sum())\n",
    "print(f\"HDBSCAN found {n_clusters} clusters (excluding -1). Noise points: {n_noise}\")\n",
    "\n",
    "# -------------------------\n",
    "# BERTopic (KeyBERTInspired + TF-IDF) — c-TF-IDF used internally\n",
    "# -------------------------\n",
    "print(\"Building BERTopic model (KeyBERTInspired + TF-IDF)...\")\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.85, min_df=5)\n",
    "representation = KeyBERTInspired()\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    representation_model=representation,\n",
    "    top_n_words=TOP_N_WORDS,\n",
    "    calculate_probabilities=CALCULATE_PROBABILITIES,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Fitting BERTopic (this will extract topics using c-TF-IDF)...\")\n",
    "topics, probs = topic_model.fit_transform(docs_clean, embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Print topic info\n",
    "# -------------------------\n",
    "print(\"\\n=== TOPIC INFO (top rows) ===\")\n",
    "print(topic_model.get_topic_info().head(15).to_string(index=False))\n",
    "\n",
    "# -------------------------\n",
    "# Coherence\n",
    "# -------------------------\n",
    "# ---- Tokenize docs for coherence ----\n",
    "tokenized_docs = [simple_preprocess(doc) for doc in docs_clean]\n",
    "\n",
    "# ---- Extract top words from each topic for coherence ----\n",
    "topics_dict = topic_model.get_topics()\n",
    "topic_words = []\n",
    "for tid, words in topics_dict.items():\n",
    "    if tid != -1:\n",
    "        topic_words.append([w for w, _ in words[:TOP_N_WORDS]])\n",
    "\n",
    "# ---- Build dictionary and compute coherence ----\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "cm = CoherenceModel(topics=topic_words, texts=tokenized_docs,\n",
    "                    dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "print(\"Overall Coherence (c_v):\", round(cm.get_coherence(), 4))\n",
    "\n",
    "# -------------------------\n",
    "# Visualizations (display only, no saving)\n",
    "# - plot_topic_size, plot_hierarchy, plot_barchart -> Plotly figures .show()\n",
    "# - visualize_topics and visualize_documents -> Plotly .show()\n",
    "# - WordClouds -> matplotlib plt.show()\n",
    "# -------------------------\n",
    "# 1) barchart\n",
    "fig_b = visualize_barchart(topic_model, top_n_topics=15)\n",
    "fig_b.show()\n",
    "\n",
    "# 2) hierarchy\n",
    "fig_h = visualize_hierarchy(topic_model)\n",
    "fig_h.show()\n",
    "\n",
    "# 3) topic map\n",
    "fig_t = visualize_topics(topic_model)\n",
    "fig_t.show()\n",
    "\n",
    "# 4) documents (embedding space)\n",
    "fig_d = visualize_documents(topic_model, docs_clean)   # or pass embeddings if supported\n",
    "fig_d.show()\n",
    "\n",
    "# WordClouds for top 5 non-outlier topics\n",
    "print(\"\\nDisplaying WordClouds for top topics...\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "top_topics = topic_info[topic_info.Topic != -1].head(5).Topic.tolist()\n",
    "for tid in top_topics:\n",
    "    words_freq = dict(topic_model.get_topic(int(tid)))\n",
    "    if not words_freq:\n",
    "        continue\n",
    "    wc = WordCloud(width=900, height=400, background_color=\"white\").generate_from_frequencies(words_freq)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"WordCloud — Topic {tid}\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
